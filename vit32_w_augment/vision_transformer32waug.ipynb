{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-22 15:43:05.801445: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "import PIL.Image\n",
    "import glob, warnings\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from datasets import load_dataset\n",
    "from transformers import ViTFeatureExtractor, AutoModelForImageClassification\n",
    "from datasets import load_metric\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.autograd import Variable\n",
    "try:\n",
    "    from torch.hub import load_state_dict_from_url\n",
    "except ImportError:\n",
    "    from torch.utils.model_zoo import load_url as load_state_dict_from_url\n",
    "\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "     \n",
    "\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d542c62fa18e492fb999ce79ba36d5d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/2600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration train-5ba040123c4f7080\n",
      "Found cached dataset imagefolder (/home/chash345/.cache/huggingface/datasets/imagefolder/train-5ba040123c4f7080/0.0.0/37fbb85cc714a338bea574ac6c7d0b5be5aff46c1862c1989b20e0771199e93f)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4c636afe55d45e882dc286a720157ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "106a1ec727944b88a70e03aee72f3e73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/870 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration valid-1603420759c35bdb\n",
      "Found cached dataset imagefolder (/home/chash345/.cache/huggingface/datasets/imagefolder/valid-1603420759c35bdb/0.0.0/37fbb85cc714a338bea574ac6c7d0b5be5aff46c1862c1989b20e0771199e93f)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e43c9e8a5388464984092e9427bfb2f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2281fa0913d94aa8afccb5f79388cfac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/864 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration test-1f68a239285f9c45\n",
      "Found cached dataset imagefolder (/home/chash345/.cache/huggingface/datasets/imagefolder/test-1f68a239285f9c45/0.0.0/37fbb85cc714a338bea574ac6c7d0b5be5aff46c1862c1989b20e0771199e93f)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffe055778b62430a8a19d2c42633a254",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train = load_dataset('/local/data1/chash345/train')\n",
    "valid = load_dataset('/local/data1/chash345/valid')\n",
    "test = load_dataset('/local/data1/chash345/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2080"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.count_nonzero(train['train']['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(train['train']['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=2990x2990>,\n",
       " 'label': 1}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['train'][2555]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTFeatureExtractor {\n",
       "  \"do_normalize\": true,\n",
       "  \"do_resize\": true,\n",
       "  \"feature_extractor_type\": \"ViTFeatureExtractor\",\n",
       "  \"image_mean\": [\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5\n",
       "  ],\n",
       "  \"image_std\": [\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5\n",
       "  ],\n",
       "  \"resample\": 2,\n",
       "  \"size\": 224\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name_or_path = 'google/vit-base-patch32-224-in21k'\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(model_name_or_path)\n",
    "\n",
    "feature_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_feature = feature_extractor(\n",
    "    train['train'][100]['image'],\n",
    "    return_tensors = 'pt'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pixel_values': tensor([[[[-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          ...,\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.]],\n",
       "\n",
       "         [[-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          ...,\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.]],\n",
       "\n",
       "         [[-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          ...,\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.]]]])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 224, 224])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_feature['pixel_values'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = torch.FloatTensor([0., 1., 2.])\n",
    "X_train.is_cuda\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'transform'=<function train_transforms at 0x7fcfa6bd3f70> of the transform datasets.arrow_dataset.Dataset.set_format couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['image', 'label'],\n",
       "    num_rows: 864\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision.transforms import (CenterCrop, \n",
    "                                    Compose, \n",
    "                                    Normalize, \n",
    "                                    RandomHorizontalFlip,\n",
    "                                    RandomResizedCrop, \n",
    "                                    Resize, \n",
    "                                    ToTensor)\n",
    "\n",
    "# %%\n",
    "normalize = Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std)\n",
    "_train_transforms = Compose(\n",
    "        [\n",
    "            RandomResizedCrop(feature_extractor.size),\n",
    "            RandomHorizontalFlip(),\n",
    "            ToTensor(),\n",
    "            normalize,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "_val_transforms = Compose(\n",
    "        [\n",
    "            Resize(feature_extractor.size),\n",
    "            CenterCrop(feature_extractor.size),\n",
    "            ToTensor(),\n",
    "            normalize,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "_test_transforms = Compose(\n",
    "        [\n",
    "            Resize(feature_extractor.size),\n",
    "            CenterCrop(feature_extractor.size),\n",
    "            ToTensor(),\n",
    "            normalize,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "def train_transforms(examples):\n",
    "    examples['pixel_values'] = [_train_transforms(image.convert(\"RGB\")) for image in examples['image']]\n",
    "    return examples\n",
    "\n",
    "def val_transforms(examples):\n",
    "    examples['pixel_values'] = [_val_transforms(image.convert(\"RGB\")) for image in examples['image']]\n",
    "    return examples\n",
    "\n",
    "def test_transforms(examples):\n",
    "    examples['pixel_values'] = [_test_transforms(image.convert(\"RGB\")) for image in examples['image']]\n",
    "    return examples\n",
    "\n",
    "# %%\n",
    "prepared_train = train['train'].with_transform(train_transforms)\n",
    "prepared_valid = valid['train'].with_transform(val_transforms)\n",
    "prepared_test = test['train'].with_transform(test_transforms)\n",
    "\n",
    "# %%\n",
    "prepared_train\n",
    "\n",
    "# %%\n",
    "prepared_valid\n",
    "\n",
    "# %%\n",
    "prepared_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepared_test.set_format(type=prepared_test.format[\"type\"], columns=list(prepared_test.features.keys()), transform= preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return{\n",
    "        'pixel_values':torch.stack([x['pixel_values'] for x in batch]),\n",
    "        'labels': torch.tensor([x['label'] for x in batch])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load_metric('accuracy')\n",
    "\n",
    "def compute_metrics(p):\n",
    "    return metric.compute(\n",
    "        predictions = np.argmax(p.predictions, axis=1),\n",
    "        references = p.label_ids\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir= '/local/data1/chash345/vit32_w_augment_model',\n",
    "    seed=100,\n",
    "    per_device_train_batch_size=16,\n",
    "    evaluation_strategy='steps',\n",
    "    num_train_epochs=15,\n",
    "    save_steps=100,\n",
    "    eval_steps=100,\n",
    "    logging_steps=10,\n",
    "    learning_rate=2e-4,\n",
    "    save_total_limit=2,\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=False,\n",
    "    load_best_model_at_end=True,\n",
    "    dataloader_pin_memory=False\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/vit-base-patch32-224-in21k were not used when initializing ViTForImageClassification: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing ViTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch32-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViTForImageClassification\n",
    "\n",
    "labels = train['train']['label']\n",
    "\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    num_labels = len(labels)\n",
    ").to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=prepared_train,\n",
    "    eval_dataset=prepared_valid,\n",
    "    tokenizer=feature_extractor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 2600\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 815\n",
      "  Number of trainable parameters = 87798056\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='815' max='815' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [815/815 2:27:29, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.708800</td>\n",
       "      <td>0.657291</td>\n",
       "      <td>0.798851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.429700</td>\n",
       "      <td>0.492501</td>\n",
       "      <td>0.808046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.542700</td>\n",
       "      <td>0.493088</td>\n",
       "      <td>0.798851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.298600</td>\n",
       "      <td>0.426059</td>\n",
       "      <td>0.832184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.159800</td>\n",
       "      <td>0.396225</td>\n",
       "      <td>0.840230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.156600</td>\n",
       "      <td>0.440261</td>\n",
       "      <td>0.855172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.044100</td>\n",
       "      <td>0.498598</td>\n",
       "      <td>0.850575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.122500</td>\n",
       "      <td>0.503367</td>\n",
       "      <td>0.845977</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 870\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../checkpoint-100\n",
      "Configuration saved in ../checkpoint-100/config.json\n",
      "Model weights saved in ../checkpoint-100/pytorch_model.bin\n",
      "Image processor saved in ../checkpoint-100/preprocessor_config.json\n",
      "Deleting older checkpoint [../checkpoint-500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 870\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../checkpoint-200\n",
      "Configuration saved in ../checkpoint-200/config.json\n",
      "Model weights saved in ../checkpoint-200/pytorch_model.bin\n",
      "Image processor saved in ../checkpoint-200/preprocessor_config.json\n",
      "Deleting older checkpoint [../checkpoint-800] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 870\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../checkpoint-300\n",
      "Configuration saved in ../checkpoint-300/config.json\n",
      "Model weights saved in ../checkpoint-300/pytorch_model.bin\n",
      "Image processor saved in ../checkpoint-300/preprocessor_config.json\n",
      "Deleting older checkpoint [../checkpoint-100] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 870\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../checkpoint-400\n",
      "Configuration saved in ../checkpoint-400/config.json\n",
      "Model weights saved in ../checkpoint-400/pytorch_model.bin\n",
      "Image processor saved in ../checkpoint-400/preprocessor_config.json\n",
      "Deleting older checkpoint [../checkpoint-200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 870\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../checkpoint-500\n",
      "Configuration saved in ../checkpoint-500/config.json\n",
      "Model weights saved in ../checkpoint-500/pytorch_model.bin\n",
      "Image processor saved in ../checkpoint-500/preprocessor_config.json\n",
      "Deleting older checkpoint [../checkpoint-300] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 870\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../checkpoint-600\n",
      "Configuration saved in ../checkpoint-600/config.json\n",
      "Model weights saved in ../checkpoint-600/pytorch_model.bin\n",
      "Image processor saved in ../checkpoint-600/preprocessor_config.json\n",
      "Deleting older checkpoint [../checkpoint-400] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 870\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../checkpoint-700\n",
      "Configuration saved in ../checkpoint-700/config.json\n",
      "Model weights saved in ../checkpoint-700/pytorch_model.bin\n",
      "Image processor saved in ../checkpoint-700/preprocessor_config.json\n",
      "Deleting older checkpoint [../checkpoint-600] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 870\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../checkpoint-800\n",
      "Configuration saved in ../checkpoint-800/config.json\n",
      "Model weights saved in ../checkpoint-800/pytorch_model.bin\n",
      "Image processor saved in ../checkpoint-800/preprocessor_config.json\n",
      "Deleting older checkpoint [../checkpoint-700] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ../checkpoint-500 (score: 0.3962247371673584).\n",
      "Saving model checkpoint to ../\n",
      "Configuration saved in ../config.json\n",
      "Model weights saved in ../pytorch_model.bin\n",
      "Image processor saved in ../preprocessor_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =         5.0\n",
      "  total_flos               = 960056791GF\n",
      "  train_loss               =       0.608\n",
      "  train_runtime            =  2:27:38.94\n",
      "  train_samples_per_second =       1.467\n",
      "  train_steps_per_second   =       0.092\n"
     ]
    }
   ],
   "source": [
    "model_results = trainer.train()\n",
    "\n",
    "trainer.save_model()\n",
    "trainer.log_metrics('train', model_results.metrics)\n",
    "trainer.save_metrics('train', model_results.metrics)\n",
    "\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /local/data1/chash345/vit32_w_augment_model/checkpoint-1600/config.json\n",
      "Model config ViTConfig {\n",
      "  \"_name_or_path\": \"google/vit-base-patch32-224-in21k\",\n",
      "  \"architectures\": [\n",
      "    \"ViTForImageClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"image_size\": 224,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 32,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"qkv_bias\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\"\n",
      "}\n",
      "\n",
      "loading weights file /local/data1/chash345/vit32_w_augment_model/checkpoint-1600/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing ViTForImageClassification.\n",
      "\n",
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at /local/data1/chash345/vit32_w_augment_model/checkpoint-1600 and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([2600, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([2600]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "using `logging_steps` to initialize `eval_steps` to 500\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 864\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = ViTForImageClassification.from_pretrained('/local/data1/chash345/vit32_w_augment_model/checkpoint-1600', num_labels=2, ignore_mismatched_sizes=True )\n",
    "\n",
    "    \n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir= '/local/data1/chash345/vit32_w_augment_model/checkpoint-1600',\n",
    "    per_device_train_batch_size=1,\n",
    "    num_train_epochs=1,\n",
    "    evaluation_strategy='steps',\n",
    "    save_strategy='steps',\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=False,\n",
    "    load_best_model_at_end=True,\n",
    "    do_predict=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=feature_extractor,\n",
    ")\n",
    "#trainer = Trainer(model=model)\n",
    "#trainer.model = model.cuda()\n",
    "prediction_test = trainer.predict(prepared_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[ 0.10793085,  0.06180411],\n",
       "       [-0.04312357,  0.0642143 ],\n",
       "       [ 0.12281008,  0.09997482],\n",
       "       ...,\n",
       "       [-0.05614903,  0.06342947],\n",
       "       [-0.09410788, -0.00248896],\n",
       "       [-0.07910109,  0.01487855]], dtype=float32), label_ids=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1]), metrics={'test_loss': 0.6529922485351562, 'test_accuracy': 0.8333333333333334, 'test_runtime': 148.8628, 'test_samples_per_second': 5.804, 'test_steps_per_second': 0.726})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-22 15:46:46.610499: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-12-22 15:46:46.613900: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2022-12-22 15:46:46.632451: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-12-22 15:46:46.632527: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: lnx00273.ad.liu.se\n",
      "2022-12-22 15:46:46.632547: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: lnx00273.ad.liu.se\n",
      "2022-12-22 15:46:46.632729: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 525.60.11\n",
      "2022-12-22 15:46:46.632815: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 525.60.11\n",
      "2022-12-22 15:46:46.632833: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 525.60.11\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "# Hide GPU from visible devices\n",
    "tf.config.set_visible_devices([], 'GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-22 15:46:47.040112: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-22 15:46:47.040534: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n"
     ]
    }
   ],
   "source": [
    "prediction = tf.round(tf.nn.sigmoid(prediction_test.predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction\n",
    "prediction_test = np.argmax(prediction, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = test['train']['label']\n",
    "y_pred = prediction_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 73, 100],\n",
       "       [ 45, 646]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_true= y_true , y_pred=y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.618644</td>\n",
       "      <td>0.421965</td>\n",
       "      <td>0.501718</td>\n",
       "      <td>173.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.865952</td>\n",
       "      <td>0.934877</td>\n",
       "      <td>0.899095</td>\n",
       "      <td>691.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.832176</td>\n",
       "      <td>0.832176</td>\n",
       "      <td>0.832176</td>\n",
       "      <td>0.832176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.742298</td>\n",
       "      <td>0.678421</td>\n",
       "      <td>0.700407</td>\n",
       "      <td>864.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.816433</td>\n",
       "      <td>0.832176</td>\n",
       "      <td>0.819528</td>\n",
       "      <td>864.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score     support\n",
       "0              0.618644  0.421965  0.501718  173.000000\n",
       "1              0.865952  0.934877  0.899095  691.000000\n",
       "accuracy       0.832176  0.832176  0.832176    0.832176\n",
       "macro avg      0.742298  0.678421  0.700407  864.000000\n",
       "weighted avg   0.816433  0.832176  0.819528  864.000000"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(classification_report(y_true, y_pred, output_dict=True)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, RocCurveDisplay, auc\n",
    "\n",
    "# %%\n",
    "fpr, tpr, thresholds = roc_curve(y_true, prediction_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6784211538944145"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%\n",
    "roc_auc_score(y_true , prediction_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc = auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "display = RocCurveDisplay(fpr=fpr,tpr=tpr, roc_auc=roc_auc)\n",
    "display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "researchkernel",
   "language": "python",
   "name": "researchkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "24745f2cfe87266a2e3f1c3bc0099ef3874c0fb00fd483dd9f076f273543ffa9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
