{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-22 09:57:02.154317: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "import PIL.Image\n",
    "import glob, warnings\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from datasets import load_dataset\n",
    "from transformers import ViTFeatureExtractor, AutoModelForImageClassification\n",
    "from datasets import load_metric\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.autograd import Variable\n",
    "try:\n",
    "    from torch.hub import load_state_dict_from_url\n",
    "except ImportError:\n",
    "    from torch.utils.model_zoo import load_url as load_state_dict_from_url\n",
    "\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "     \n",
    "\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5596514df0eb47c2a253001a5f49f294",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/2600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration train-5ba040123c4f7080\n",
      "Found cached dataset imagefolder (/home/chash345/.cache/huggingface/datasets/imagefolder/train-5ba040123c4f7080/0.0.0/37fbb85cc714a338bea574ac6c7d0b5be5aff46c1862c1989b20e0771199e93f)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d055940ce5bf47e380d0d3fb39999b16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a70df8688d348568d778b39df7767f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/870 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration valid-1603420759c35bdb\n",
      "Found cached dataset imagefolder (/home/chash345/.cache/huggingface/datasets/imagefolder/valid-1603420759c35bdb/0.0.0/37fbb85cc714a338bea574ac6c7d0b5be5aff46c1862c1989b20e0771199e93f)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9f43ebd96a04ec28e74ed5a27863212",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "834fda95e7df47e5848c9b5139d6b0f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/864 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration test-1f68a239285f9c45\n",
      "Found cached dataset imagefolder (/home/chash345/.cache/huggingface/datasets/imagefolder/test-1f68a239285f9c45/0.0.0/37fbb85cc714a338bea574ac6c7d0b5be5aff46c1862c1989b20e0771199e93f)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33ccbe25d18a43ab886c747141e659c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train = load_dataset('/local/data1/chash345/train')\n",
    "valid = load_dataset('/local/data1/chash345/valid')\n",
    "test = load_dataset('/local/data1/chash345/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2080"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.count_nonzero(train['train']['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(train['train']['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=2990x2990>,\n",
       " 'label': 1}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['train'][2555]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTFeatureExtractor {\n",
       "  \"do_normalize\": true,\n",
       "  \"do_resize\": true,\n",
       "  \"feature_extractor_type\": \"ViTFeatureExtractor\",\n",
       "  \"image_mean\": [\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5\n",
       "  ],\n",
       "  \"image_std\": [\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5\n",
       "  ],\n",
       "  \"resample\": 2,\n",
       "  \"size\": 224\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name_or_path = 'google/vit-base-patch16-224-in21k'\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(model_name_or_path)\n",
    "\n",
    "feature_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_feature = feature_extractor(\n",
    "    train['train'][100]['image'],\n",
    "    return_tensors = 'pt'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pixel_values': tensor([[[[-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          ...,\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.]],\n",
       "\n",
       "         [[-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          ...,\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.]],\n",
       "\n",
       "         [[-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          ...,\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.]]]])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 224, 224])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_feature['pixel_values'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(batch):\n",
    "    inputs = feature_extractor(\n",
    "        batch['image'],\n",
    "        return_tensors = 'pt'\n",
    "    ).to(device)\n",
    "\n",
    "    inputs['label'] = batch['label']\n",
    "\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'transform'=<function preprocess at 0x7f9e600f4d30> of the transform datasets.arrow_dataset.Dataset.set_format couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    }
   ],
   "source": [
    "prepared_train = train['train'].with_transform(preprocess)\n",
    "prepared_valid = valid['train'].with_transform(preprocess)\n",
    "prepared_test = test['train'].with_transform(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'custom',\n",
       " 'format_kwargs': {'transform': <function __main__.preprocess(batch)>},\n",
       " 'columns': ['image', 'label'],\n",
       " 'output_all_columns': False}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepared_test.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': Image(decode=True, id=None),\n",
       " 'label': ClassLabel(names=['aff', 'control'], id=None)}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepared_test.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_test.set_format(type=prepared_test.format[\"type\"], columns=list(prepared_test.features.keys()), transform= preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'custom',\n",
       " 'format_kwargs': {'transform': <function __main__.preprocess(batch)>},\n",
       " 'columns': ['image', 'label'],\n",
       " 'output_all_columns': False}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepared_test.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return{\n",
    "        'pixel_values':torch.stack([x['pixel_values'] for x in batch]),\n",
    "        'labels': torch.tensor([x['label'] for x in batch])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load_metric('accuracy')\n",
    "\n",
    "def compute_metrics(p):\n",
    "    return metric.compute(\n",
    "        predictions = np.argmax(p.predictions, axis=1),\n",
    "        references = p.label_ids\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir= '/local/data1/chash345/Vision-Transformer-Research-Project/vit16_w_o_augment',\n",
    "    seed=42,\n",
    "    per_device_train_batch_size=16,\n",
    "    evaluation_strategy='steps',\n",
    "    num_train_epochs=10,\n",
    "    save_steps=200,\n",
    "    eval_steps=200,\n",
    "    logging_steps=10,\n",
    "    learning_rate=1e-4,\n",
    "    save_total_limit=2,\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=False,\n",
    "    load_best_model_at_end=True,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing ViTForImageClassification: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing ViTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViTForImageClassification\n",
    "\n",
    "labels = train['train']['label']\n",
    "\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    num_labels = len(labels)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=prepared_train,\n",
    "    eval_dataset=prepared_valid,\n",
    "    tokenizer=feature_extractor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 2600\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 815\n",
      "  Number of trainable parameters = 87798056\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='815' max='815' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [815/815 2:27:29, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.708800</td>\n",
       "      <td>0.657291</td>\n",
       "      <td>0.798851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.429700</td>\n",
       "      <td>0.492501</td>\n",
       "      <td>0.808046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.542700</td>\n",
       "      <td>0.493088</td>\n",
       "      <td>0.798851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.298600</td>\n",
       "      <td>0.426059</td>\n",
       "      <td>0.832184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.159800</td>\n",
       "      <td>0.396225</td>\n",
       "      <td>0.840230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.156600</td>\n",
       "      <td>0.440261</td>\n",
       "      <td>0.855172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.044100</td>\n",
       "      <td>0.498598</td>\n",
       "      <td>0.850575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.122500</td>\n",
       "      <td>0.503367</td>\n",
       "      <td>0.845977</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 870\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../checkpoint-100\n",
      "Configuration saved in ../checkpoint-100/config.json\n",
      "Model weights saved in ../checkpoint-100/pytorch_model.bin\n",
      "Image processor saved in ../checkpoint-100/preprocessor_config.json\n",
      "Deleting older checkpoint [../checkpoint-500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 870\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../checkpoint-200\n",
      "Configuration saved in ../checkpoint-200/config.json\n",
      "Model weights saved in ../checkpoint-200/pytorch_model.bin\n",
      "Image processor saved in ../checkpoint-200/preprocessor_config.json\n",
      "Deleting older checkpoint [../checkpoint-800] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 870\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../checkpoint-300\n",
      "Configuration saved in ../checkpoint-300/config.json\n",
      "Model weights saved in ../checkpoint-300/pytorch_model.bin\n",
      "Image processor saved in ../checkpoint-300/preprocessor_config.json\n",
      "Deleting older checkpoint [../checkpoint-100] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 870\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../checkpoint-400\n",
      "Configuration saved in ../checkpoint-400/config.json\n",
      "Model weights saved in ../checkpoint-400/pytorch_model.bin\n",
      "Image processor saved in ../checkpoint-400/preprocessor_config.json\n",
      "Deleting older checkpoint [../checkpoint-200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 870\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../checkpoint-500\n",
      "Configuration saved in ../checkpoint-500/config.json\n",
      "Model weights saved in ../checkpoint-500/pytorch_model.bin\n",
      "Image processor saved in ../checkpoint-500/preprocessor_config.json\n",
      "Deleting older checkpoint [../checkpoint-300] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 870\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../checkpoint-600\n",
      "Configuration saved in ../checkpoint-600/config.json\n",
      "Model weights saved in ../checkpoint-600/pytorch_model.bin\n",
      "Image processor saved in ../checkpoint-600/preprocessor_config.json\n",
      "Deleting older checkpoint [../checkpoint-400] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 870\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../checkpoint-700\n",
      "Configuration saved in ../checkpoint-700/config.json\n",
      "Model weights saved in ../checkpoint-700/pytorch_model.bin\n",
      "Image processor saved in ../checkpoint-700/preprocessor_config.json\n",
      "Deleting older checkpoint [../checkpoint-600] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 870\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../checkpoint-800\n",
      "Configuration saved in ../checkpoint-800/config.json\n",
      "Model weights saved in ../checkpoint-800/pytorch_model.bin\n",
      "Image processor saved in ../checkpoint-800/preprocessor_config.json\n",
      "Deleting older checkpoint [../checkpoint-700] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ../checkpoint-500 (score: 0.3962247371673584).\n",
      "Saving model checkpoint to ../\n",
      "Configuration saved in ../config.json\n",
      "Model weights saved in ../pytorch_model.bin\n",
      "Image processor saved in ../preprocessor_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =         5.0\n",
      "  total_flos               = 960056791GF\n",
      "  train_loss               =       0.608\n",
      "  train_runtime            =  2:27:38.94\n",
      "  train_samples_per_second =       1.467\n",
      "  train_steps_per_second   =       0.092\n"
     ]
    }
   ],
   "source": [
    "model_results = trainer.train()\n",
    "\n",
    "trainer.save_model()\n",
    "trainer.log_metrics('train', model_results.metrics)\n",
    "trainer.save_metrics('train', model_results.metrics)\n",
    "\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /local/data1/chash345/vit16_w_o_augment_model/checkpoint-1600/config.json\n",
      "Model config ViTConfig {\n",
      "  \"_name_or_path\": \"google/vit-base-patch16-224-in21k\",\n",
      "  \"architectures\": [\n",
      "    \"ViTForImageClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"image_size\": 224,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"qkv_bias\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\"\n",
      "}\n",
      "\n",
      "loading weights file /local/data1/chash345/vit16_w_o_augment_model/checkpoint-1600/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing ViTForImageClassification.\n",
      "\n",
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at /local/data1/chash345/vit16_w_o_augment_model/checkpoint-1600 and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([2600, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([2600]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "using `logging_steps` to initialize `eval_steps` to 500\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 864\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = ViTForImageClassification.from_pretrained('/local/data1/chash345/vit16_w_o_augment_model/checkpoint-1600', num_labels=2, ignore_mismatched_sizes=True )\n",
    "    #label2id={\"aff\": 0, \"control\": 1},\n",
    "    #id2label={0: \"aff\", 1: \"control\"},\n",
    "    \n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir= '/local/data1/chash345/vit16_w_o_augment_model/checkpoint-1600',\n",
    "    seed=42,\n",
    "    per_device_train_batch_size=1,\n",
    "    num_train_epochs=1,\n",
    "    evaluation_strategy='steps',\n",
    "    save_strategy='steps',\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=False,\n",
    "    load_best_model_at_end=True,\n",
    "    do_predict=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=feature_extractor,\n",
    ")\n",
    "#trainer = Trainer(model=model)\n",
    "#trainer.model = model.cuda()\n",
    "prediction_test = trainer.predict(prepared_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[ 0.09055269, -0.05189572],\n",
       "       [-0.12026437, -0.00272117],\n",
       "       [ 0.08647573, -0.06566948],\n",
       "       ...,\n",
       "       [-0.13996002,  0.0007032 ],\n",
       "       [-0.13295633,  0.00150126],\n",
       "       [-0.13624734,  0.01540528]], dtype=float32), label_ids=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1]), metrics={'test_loss': 0.6388602256774902, 'test_accuracy': 0.8993055555555556, 'test_runtime': 219.8161, 'test_samples_per_second': 3.931, 'test_steps_per_second': 0.491})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-22 10:22:03.559246: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-12-22 10:22:03.562824: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2022-12-22 10:22:03.581614: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-12-22 10:22:03.581696: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: lnx00273.ad.liu.se\n",
      "2022-12-22 10:22:03.581714: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: lnx00273.ad.liu.se\n",
      "2022-12-22 10:22:03.581897: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 525.60.11\n",
      "2022-12-22 10:22:03.581973: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 525.60.11\n",
      "2022-12-22 10:22:03.581989: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 525.60.11\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "# Hide GPU from visible devices\n",
    "tf.config.set_visible_devices([], 'GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-22 10:22:06.714048: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-22 10:22:06.714565: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n"
     ]
    }
   ],
   "source": [
    "prediction = tf.round(tf.nn.sigmoid(prediction_test.predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction\n",
    "prediction_test = np.argmax(prediction, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = test['train']['label']\n",
    "y_pred = prediction_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[138,  35],\n",
       "       [126, 565]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_true= y_true , y_pred=y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.522727</td>\n",
       "      <td>0.797688</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>173.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.941667</td>\n",
       "      <td>0.817656</td>\n",
       "      <td>0.875290</td>\n",
       "      <td>691.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.813657</td>\n",
       "      <td>0.813657</td>\n",
       "      <td>0.813657</td>\n",
       "      <td>0.813657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.732197</td>\n",
       "      <td>0.807672</td>\n",
       "      <td>0.753435</td>\n",
       "      <td>864.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.857782</td>\n",
       "      <td>0.813657</td>\n",
       "      <td>0.826492</td>\n",
       "      <td>864.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score     support\n",
       "0              0.522727  0.797688  0.631579  173.000000\n",
       "1              0.941667  0.817656  0.875290  691.000000\n",
       "accuracy       0.813657  0.813657  0.813657    0.813657\n",
       "macro avg      0.732197  0.807672  0.753435  864.000000\n",
       "weighted avg   0.857782  0.813657  0.826492  864.000000"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(classification_report(y_true, y_pred, output_dict=True)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, RocCurveDisplay, auc\n",
    "\n",
    "# %%\n",
    "fpr, tpr, thresholds = roc_curve(y_true, prediction_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8076717164534937"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%\n",
    "roc_auc_score(y_true , prediction_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc = auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "display = RocCurveDisplay(fpr=fpr,tpr=tpr, roc_auc=roc_auc)\n",
    "display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We see that Vision Transformer beats all other CNN models and gets the accuracy of around 90% on test set and AUROC of 0.81, considering a highly imabalanced dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "researchkernel",
   "language": "python",
   "name": "researchkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "24745f2cfe87266a2e3f1c3bc0099ef3874c0fb00fd483dd9f076f273543ffa9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
